{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Assignment_3(2).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyJ25uz0kSaw"
      },
      "source": [
        "# Assignment 3 on Natural Language Processing\n",
        "\n",
        "## Date : 30th Sept, 2020\n",
        "\n",
        "### Instructor : Prof. Sudeshna Sarkar\n",
        "\n",
        "### Teaching Assistants : Alapan Kuila, Aniruddha Roy, Anusha Potnuru, Uppada Vishnu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ao1nhg9RknmF"
      },
      "source": [
        "The central idea of this assignment is to use Naive Bayes classifier and LSTM based classifier and compare the models by accuracy on IMDB dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONM5Q4SCe9Mr"
      },
      "source": [
        "Please submit with outputs. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElRkQElWUMjG"
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import keras\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhHRim2AUm4z"
      },
      "source": [
        "#Load the IMDB dataset. You can load it using pandas as dataframe\n",
        "df = pd.read_csv(\"IMDB Dataset.csv\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCl5aVsTdCOl",
        "outputId": "c2739d74-11ee-449f-94c0-dd89d7b8f48e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3_21EmadDc9",
        "outputId": "380ba5fb-5d58-404e-a895-d3e4fac360fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        }
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>50000</td>\n",
              "      <td>50000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>49582</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>Loved today's show!!! It was a variety and not...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>5</td>\n",
              "      <td>25000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   review sentiment\n",
              "count                                               50000     50000\n",
              "unique                                              49582         2\n",
              "top     Loved today's show!!! It was a variety and not...  positive\n",
              "freq                                                    5     25000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRx9GsKxdFGW",
        "outputId": "247b18e6-d22b-4a98-b82a-fddd3cbab697",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "df['sentiment'].value_counts()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "positive    25000\n",
              "negative    25000\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lK_Hn2f6VMP7"
      },
      "source": [
        "# Preprocessing\n",
        "PrePrecessing that needs to be done on lower cased corpus\n",
        "\n",
        "1. Remove html tags\n",
        "2. Remove URLS\n",
        "3. Remove non alphanumeric character\n",
        "4. Remove Stopwords\n",
        "5. Perform stemming and lemmatization\n",
        "\n",
        "You can use regex from re. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5B5lHZPsVOXv"
      },
      "source": [
        "def lower_case(raw_text):\n",
        "  return raw_text.lower()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShlLSGYNdNxi"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "def clean_html(raw_text):\n",
        "  clean_text = BeautifulSoup(raw_text, \"lxml\").text\n",
        "  return clean_text"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7lH-Hd2dN0d"
      },
      "source": [
        "def clean_URLs(raw_text):\n",
        "  clean_text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', raw_text, flags=re.MULTILINE)\n",
        "  return clean_text"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAyBaT8WdN2y"
      },
      "source": [
        "def clean_non_alpha(raw_text):\n",
        "  return re.sub(r'[^a-zA-z0-9\\s]', '', raw_text)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vp3a5XJdN5o",
        "outputId": "3d290c7a-8f7c-49aa-a08c-b7afc0544ce1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RDE-Lw8dN7m"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "tokenizer=ToktokTokenizer()\n",
        "\n",
        "stopwords_list=stopwords.words('english')\n",
        "#print(stopwords_list)\n",
        "\n",
        "def remove_stopwords(raw_text):\n",
        "  tokens = tokenizer.tokenize(raw_text)\n",
        "  clean_tokens = [token for token in tokens if token not in stopwords_list]\n",
        "  clean_text = ' '.join(clean_tokens) \n",
        "  return clean_text"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YO2a0vDmdN-z"
      },
      "source": [
        "def stemming(text):\n",
        "  ps=nltk.porter.PorterStemmer()\n",
        "  text= ' '.join([ps.stem(word) for word in text.split()])\n",
        "  return text"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtQFOhATdZHC"
      },
      "source": [
        "df['review']=df['review'].apply(lower_case)\n",
        "df['review']=df['review'].apply(clean_html)\n",
        "df['review']=df['review'].apply(clean_URLs)\n",
        "df['review']=df['review'].apply(clean_non_alpha)\n",
        "df['review']=df['review'].apply(remove_stopwords)\n",
        "df['review']=df['review'].apply(stemming)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beylUiZ4dZKo"
      },
      "source": [
        "#df['review'][0]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyaSkfcvYGXk",
        "outputId": "2fda12d6-488f-49ff-fb8a-1d97e4776864",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "# Print Statistics of Data like avg length of sentence , proposition of data w.r.t class labels\n",
        "length = [0] * df.shape[0]\n",
        "i=0\n",
        "for sentence in df['review']:\n",
        "  tokens = tokenizer.tokenize(sentence)\n",
        "  length[i] = len(tokens)\n",
        "  i+=1\n",
        "\n",
        "avg_length = sum(length)/len(length)\n",
        "max_length = max(length)\n",
        "min_length = min(length)\n",
        "\n",
        "print(\"avg length - \", avg_length)\n",
        "print(\"max length - \", max_length)\n",
        "print(\"min length - \", min_length)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "avg length -  119.8392\n",
            "max length -  1429\n",
            "min length -  0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FkJ-e2pUwun"
      },
      "source": [
        "# Naive Bayes classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVq-mN28U_J4"
      },
      "source": [
        "# get reviews column from df\n",
        "reviews = df['review']\n",
        "\n",
        "# get labels column from df\n",
        "labels = df['sentiment']"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ljo5NquhXTXr",
        "outputId": "69fc80ed-5762-4b94-bd35-3012c031a6c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "# Use label encoder to encode labels. Convert to 0/1\n",
        "encoder = LabelEncoder()\n",
        "encoded_labels = encoder.fit_transform(labels)\n",
        "\n",
        "print(encoded_labels[:10])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 1 1 0 1 1 1 0 0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzG-C_EVWWET"
      },
      "source": [
        "# Split the data into train and test (80% - 20%). \n",
        "# Use stratify in train_test_split so that both train and test have similar ratio of positive and negative samples.\n",
        "\n",
        "train_reviews, test_reviews, train_labels, test_labels = train_test_split(reviews, encoded_labels, test_size=0.2, stratify=encoded_labels)\n",
        "\n",
        "# train_sentences, test_sentences, train_labels, test_labels\n",
        "\n",
        "train_reviews = list(train_reviews)\n",
        "test_reviews = list(test_reviews)\n",
        "train_labels = list(train_labels)\n",
        "test_labels = list(test_labels)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bz1YdsSkiWCX"
      },
      "source": [
        "Here there are two approaches possible for building vocabulary for the naive Bayes.\n",
        "1. Take the whole data (train + test) to build the vocab. In this way while testing there is no word which will be out of vocabulary.\n",
        "2. Take the train data to build vocab. In this case, some words from the test set may not be in vocab and hence one needs to perform smoothing so that one the probability term is not zero.\n",
        " \n",
        "You are supposed to go by the 2nd approach.\n",
        " \n",
        "Also building vocab by taking all words in the train set is memory intensive, hence you are required to build vocab by choosing the top 2000 - 3000 frequent words in the training corpus.\n",
        "\n",
        "> $ P(x_i | w_j) = \\frac{ N_{x_i,w_j}\\, +\\, \\alpha }{ N_{w_j}\\, +\\, \\alpha*d} $\n",
        "\n",
        "\n",
        "$N_{x_i,w_j}$ : Number of times feature $x_i$ appears in samples of class $w_j$\n",
        "\n",
        "$N_{w_j}$ : Total count of features in class $w_j$\n",
        "\n",
        "$\\alpha$ : Parameter for additive smoothing. Here consider $\\alpha$ = 1\n",
        "\n",
        "$d$ : Dimentionality of the feature vector  $x = [x_1,x_2,...,x_d]$. In our case its the vocab size.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cllNfGmUr77",
        "outputId": "b50c68ad-0b07-4d3f-ce20-581343a1f722",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# Use Count vectorizer to get frequency of the words\n",
        "'''\n",
        "max_features parameter : If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.\n",
        "vec = CountVectorizer(max_features = 3000)\n",
        "X = vec.fit_transform(Sentence_list)\n",
        "'''\n",
        "\n",
        "vec = CountVectorizer(max_features = 3000)\n",
        "train_freq = vec.fit_transform(train_reviews).toarray()\n",
        "dictionary = vec.get_feature_names()\n",
        "\n",
        "print(train_freq.shape)\n",
        "#print(dictionary)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(40000, 3000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgY32bXFgCB4",
        "outputId": "7b5b4a87-3c1b-4c5b-fd45-acf28bc9f144",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "N_pos = train_labels.count(1)\n",
        "N_neg = train_labels.count(0)\n",
        "print(N_pos, N_neg)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20000 20000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHecYCotg8G4"
      },
      "source": [
        "pos_dict = {}\n",
        "neg_dict = {}\n",
        "\n",
        "for j in range(len(dictionary)):\n",
        "  pos_count=0\n",
        "  neg_count=0\n",
        "  for i in range(train_freq.shape[0]):\n",
        "    if(train_labels[i]==1):\n",
        "      pos_count += train_freq[i][j]\n",
        "    else:\n",
        "      neg_count += train_freq[i][j]\n",
        "  pos_dict[dictionary[j]] = pos_count\n",
        "  neg_dict[dictionary[j]] = neg_count\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzRvPjWaWUnm"
      },
      "source": [
        "# Use laplace smoothing for words in test set not present in vocab of train set\n",
        "# Build the model. Don't use the model from sklearn\n",
        "\n",
        "log_pos = [0] * len(test_reviews)\n",
        "log_neg = [0] * len(test_reviews)\n",
        "\n",
        "for i in range(len(test_reviews)):\n",
        "  tokens = tokenizer.tokenize(test_reviews[i])\n",
        "  for token in tokens:\n",
        "    if token in dictionary:\n",
        "      prob_pos = (pos_dict[token]+1)/(N_pos+len(dictionary))\n",
        "      prob_neg = (neg_dict[token]+1)/(N_neg+len(dictionary))\n",
        "    else:\n",
        "      prob_pos = 1 / (N_pos+len(dictionary))\n",
        "      prob_neg = 1 / (N_neg+len(dictionary))\n",
        "    log_pos[i] += np.log(prob_pos)\n",
        "    log_neg[i] += np.log(prob_neg)\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iE7pxWIYW1z0",
        "outputId": "71a0ca05-987a-41b4-8c66-62c8faabd907",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(len(log_pos))\n",
        "print(len(log_neg))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000\n",
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtQSl1zvW4DD",
        "outputId": "5a7517b2-d92c-4937-961b-22a91b93f7a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "# Test the model on test set and report Accuracy\n",
        "pred_labels = [0] * len(test_labels)\n",
        "\n",
        "for i in range(len(test_labels)):\n",
        "  pred_labels[i]=int(log_pos[i]>=log_neg[i])\n",
        "\n",
        "#print(pred_labels)\n",
        "\n",
        "accuracy = sum(x == y for x, y in zip(pred_labels, test_labels)) / len(test_labels)\n",
        "print(accuracy) \n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8388\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlNql0acU7sa"
      },
      "source": [
        "# *LSTM* based Classifier\n",
        "\n",
        "Use the above train and test splits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkqnvbUOXoN0"
      },
      "source": [
        "# Hyperparameters of the model\n",
        "vocab_size = len(dictionary) # choose based on statistics\n",
        "oov_tok = '<OOK>'\n",
        "embedding_dim = 100\n",
        "max_length = 1500 # choose based on statistics, for example 150 to 200\n",
        "padding_type='post'\n",
        "trunc_type='post'"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeycEg9nZAOF"
      },
      "source": [
        "# tokenize sentences\n",
        "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(train_reviews)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# convert train dataset to sequence and pad sequences\n",
        "train_sequences = tokenizer.texts_to_sequences(train_reviews)\n",
        "train_padded = pad_sequences(train_sequences, padding='post', maxlen=max_length)\n",
        "\n",
        "# convert Test dataset to sequence and pad sequences\n",
        "test_sequences = tokenizer.texts_to_sequences(test_reviews)\n",
        "test_padded = pad_sequences(test_sequences, padding='post', maxlen=max_length)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mtw3w895ZP39",
        "outputId": "7a394fb5-ecd5-459e-81c4-3f28e262ee89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        }
      },
      "source": [
        "# model initialization\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    keras.layers.Bidirectional(keras.layers.LSTM(64)),\n",
        "    keras.layers.Dense(24, activation='relu'),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# compile model\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# model summary\n",
        "model.summary()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (None, 1500, 100)         300000    \n",
            "_________________________________________________________________\n",
            "bidirectional_4 (Bidirection (None, 128)               84480     \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 24)                3096      \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 1)                 25        \n",
            "=================================================================\n",
            "Total params: 387,601\n",
            "Trainable params: 387,601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skmaDJMnZTzc",
        "outputId": "3e419cf4-50c5-41d9-a028-f64571938d2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "num_epochs = 5\n",
        "history = model.fit(train_padded, np.array(train_labels), \n",
        "                    epochs=num_epochs, verbose=1,\n",
        "                    validation_split=0.1) "
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1125/1125 [==============================] - 129s 114ms/step - loss: 0.4132 - accuracy: 0.8176 - val_loss: 0.3517 - val_accuracy: 0.8512\n",
            "Epoch 2/10\n",
            "1125/1125 [==============================] - 128s 114ms/step - loss: 0.3401 - accuracy: 0.8624 - val_loss: 0.3444 - val_accuracy: 0.8555\n",
            "Epoch 3/10\n",
            "1125/1125 [==============================] - 127s 113ms/step - loss: 0.2869 - accuracy: 0.8875 - val_loss: 0.3200 - val_accuracy: 0.8665\n",
            "Epoch 4/10\n",
            "1125/1125 [==============================] - 128s 114ms/step - loss: 0.2577 - accuracy: 0.8991 - val_loss: 0.3243 - val_accuracy: 0.8702\n",
            "Epoch 5/10\n",
            "1125/1125 [==============================] - 128s 113ms/step - loss: 0.2334 - accuracy: 0.9102 - val_loss: 0.3348 - val_accuracy: 0.8635\n",
            "Epoch 6/10\n",
            "1125/1125 [==============================] - 128s 113ms/step - loss: 0.2121 - accuracy: 0.9193 - val_loss: 0.3406 - val_accuracy: 0.8687\n",
            "Epoch 7/10\n",
            "1125/1125 [==============================] - 128s 113ms/step - loss: 0.1892 - accuracy: 0.9304 - val_loss: 0.3695 - val_accuracy: 0.8670\n",
            "Epoch 8/10\n",
            "1125/1125 [==============================] - 127s 113ms/step - loss: 0.1893 - accuracy: 0.9307 - val_loss: 0.3726 - val_accuracy: 0.8583\n",
            "Epoch 9/10\n",
            "1125/1125 [==============================] - 128s 114ms/step - loss: 0.1625 - accuracy: 0.9417 - val_loss: 0.3702 - val_accuracy: 0.8595\n",
            "Epoch 10/10\n",
            "1125/1125 [==============================] - 127s 113ms/step - loss: 0.1388 - accuracy: 0.9526 - val_loss: 0.4262 - val_accuracy: 0.8545\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjEhWEr5Zq7M",
        "outputId": "ac19bfef-d208-41c3-e3cf-a2d56e8259e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "# Calculate accuracy on Test data\n",
        "'''\n",
        "prediction = model.predict(test_padded)\n",
        "\n",
        "'''\n",
        "# Get probabilities\n",
        "probabilities = model.predict(test_padded).squeeze()\n",
        "\n",
        "# Get labels based on probability 1 if p>= 0.5 else 0\n",
        "pred_labels = probabilities>=0.5\n",
        "\n",
        "# Accuracy : one can use classification_report from sklearn\n",
        "accuracy = sum(x == y for x, y in zip(pred_labels, test_labels)) / len(test_labels)\n",
        "print(accuracy*100)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "86.31\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIICV-ySOYL0"
      },
      "source": [
        "## Get predictions for random examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2RmfNL3OYL0",
        "outputId": "8deab741-ffef-4417-bf9b-f4f70abe17f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# reviews on which we need to predict\n",
        "sentence = [\"The movie was very touching and heart whelming\", \n",
        "            \"I have never seen a terrible movie like this\", \n",
        "            \"the movie plot is terrible but it had good acting\"]\n",
        "\n",
        "# convert to a sequence\n",
        "sequences = tokenizer.texts_to_sequences(sentence)\n",
        "\n",
        "# pad the sequence\n",
        "padded = pad_sequences(sequences, padding='post', maxlen=1500)\n",
        "\n",
        "# Get probabilities\n",
        "prob = model.predict(padded).squeeze()\n",
        "print(prob)\n",
        "\n",
        "# Get labels based on probability 1 if p>= 0.5 else 0\n",
        "pred_labels = (prob>=0.5).squeeze()\n",
        "print(pred_labels)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.6004889 0.6055787 0.5217633]\n",
            "[ True  True  True]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QaygLdKxY0Q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}